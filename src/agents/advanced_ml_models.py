"""
ü§ñ ADVANCED MACHINE LEARNING MODELS
Ensemble de m√∫ltiplos modelos com 80+ features para detec√ß√£o AML/CFT
"""
from typing import Dict, Any, List, Optional, Tuple
import numpy as np
from datetime import datetime, timedelta
from decimal import Decimal
import time
from loguru import logger

from sklearn.ensemble import (
    IsolationForest, 
    RandomForestClassifier, 
    GradientBoostingClassifier,
    VotingClassifier
)
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from sklearn.covariance import EllipticEnvelope

from ..models.schemas import Transaction
from ..agents.base import BaseAgent, AgentResult


class AdvancedFeatureEngineering:
    """
    Engenharia de features avan√ßada com 80+ features
    """
    
    @staticmethod
    def extract_all_features(
        transaction: Transaction,
        customer_history: List[Transaction] = None,
        network_data: Dict[str, Any] = None
    ) -> Dict[str, float]:
        """
        Extrai 80+ features de uma transa√ß√£o
        """
        features = {}
        
        # ==================== BASIC FEATURES (10) ====================
        features['amount'] = float(transaction.amount)
        features['hour'] = transaction.timestamp.hour
        features['day_of_week'] = transaction.timestamp.weekday()
        features['day_of_month'] = transaction.timestamp.day
        features['month'] = transaction.timestamp.month
        features['is_weekend'] = 1 if transaction.timestamp.weekday() >= 5 else 0
        features['is_business_hours'] = 1 if 9 <= transaction.timestamp.hour <= 17 else 0
        features['is_night'] = 1 if transaction.timestamp.hour < 6 or transaction.timestamp.hour > 22 else 0
        features['amount_log'] = np.log1p(features['amount'])
        features['amount_sqrt'] = np.sqrt(features['amount'])
        
        # ==================== RorND AMorNT FEATURES (5) ====================
        features['is_round_amount'] = 1 if float(transaction.amount) % 1000 == 0 else 0
        features['is_very_round'] = 1 if float(transaction.amount) % 10000 == 0 else 0
        features['amount_last_digit'] = int(float(transaction.amount)) % 10
        features['amount_roundness_score'] = AdvancedFeatureEngineering._calculate_roundness(float(transaction.amount))
        features['cents_present'] = 0 if float(transaction.amount) % 1 == 0 else 1
        
        # ==================== TRANSACTION TYPand FEATURES (8) ====================
        txn_types = ['wire_transfer', 'cash_deposit', 'cash_withdrawal', 'check', 'ach', 'card_payment', 'crypto', 'international']
        for txn_type in txn_types:
            features[f'type_{txn_type}'] = 1 if transaction.transaction_type == txn_type else 0
        
        # ==================== GEO FEATURES (6) ====================
        features['is_domestic'] = 1 if transaction.country_origin == transaction.country_destination else 0
        features['is_international'] = 1 - features['is_domestic']
        
        high_risk_countries = {'IR', 'KP', 'SY', 'AF', 'IQ', 'LY', 'SO', 'SD', 'YE', 'MM'}
        features['origin_high_risk'] = 1 if transaction.country_origin in high_risk_countries else 0
        features['destination_high_risk'] = 1 if transaction.country_destination in high_risk_countries else 0
        features['any_high_risk'] = max(features['origin_high_risk'], features['destination_high_risk'])
        features['both_high_risk'] = min(features['origin_high_risk'], features['destination_high_risk'])
        
        if not customer_history:
            customer_history = []
        
        # ==================== HISTORICAL STATISTICAL FEATURES (15) ====================
        if len(customer_history) > 0:
            amounts = [float(t.amount) for t in customer_history]
            
            features['hist_count'] = len(amounts)
            features['hist_mean'] = np.mean(amounts)
            features['hist_median'] = np.median(amounts)
            features['hist_std'] = np.std(amounts) if len(amounts) > 1 else 0
            features['hist_min'] = np.min(amounts)
            features['hist_max'] = np.max(amounts)
            features['hist_q25'] = np.percentile(amounts, 25) if len(amounts) > 1 else 0
            features['hist_q75'] = np.percentile(amounts, 75) if len(amounts) > 1 else 0
            features['hist_iqr'] = features['hist_q75'] - features['hist_q25']
            features['hist_cv'] = features['hist_std'] / features['hist_mean'] if features['hist_mean'] > 0 else 0
            features['hist_skew'] = AdvancedFeatureEngineering._calculate_skewness(amounts)
            features['hist_kurtosis'] = AdvancedFeatureEngineering._calculate_kurtosis(amounts)
            
            # Z-score
            if features['hist_std'] > 0:
                features['amount_zscore'] = (features['amount'] - features['hist_mean']) / features['hist_std']
            else:
                features['amount_zscore'] = 0
            
            # Percentiland position
            features['amount_percentile'] = sum(1 for a in amounts if a <= features['amount']) / len(amounts) * 100
            
            # Distancand from mean in multiples of std
            features['std_distance'] = abs(features['amount_zscore'])
        else:
            for key in ['hist_count', 'hist_mean', 'hist_median', 'hist_std', 'hist_min', 'hist_max',
                       'hist_q25', 'hist_q75', 'hist_iqr', 'hist_cv', 'hist_skew', 'hist_kurtosis',
                       'amount_zscore', 'amount_percentile', 'std_distance']:
                features[key] = 0
        
        # ==================== VELOCITY FEATURES (12) ====================
        now = transaction.timestamp
        
        for window_days in [1, 7, 30]:
            window_txns = [t for t in customer_history if (now - t.timestamp).days <= window_days]
            
            features[f'count_{window_days}d'] = len(window_txns)
            features[f'volume_{window_days}d'] = sum(float(t.amount) for t in window_txns) if window_txns else 0
            features[f'avg_amount_{window_days}d'] = features[f'volume_{window_days}d'] / features[f'count_{window_days}d'] if features[f'count_{window_days}d'] > 0 else 0
            features[f'max_amount_{window_days}d'] = max((float(t.amount) for t in window_txns), default=0) if window_txns else 0
        
        # ==================== PATTERN FEATURES (8) ====================
        if len(customer_history) >= 3:
            # Burst oftection (muitas transa√ß√µes in curto per√≠odo)
            last_24h = [t for t in customer_history if (now - t.timestamp).total_seconds() <= 86400]
            features['burst_24h'] = len(last_24h)
            features['is_burst'] = 1 if len(last_24h) > 10 else 0
            
            # Structuring oftection (m√∫ltiplas transa√ß√µes logo abaixo do threshold)
            recent = [t for t in customer_history[-10:]]
            near_threshold = [t for t in recent if 9000 <= float(t.amount) <= 10000]
            features['structuring_count'] = len(near_threshold)
            features['is_structuring'] = 1 if len(near_threshold) >= 3 else 0
            
            # Escalation (valores crescentes)
            recent_amounts = [float(t.amount) for t in customer_history[-5:]]
            features['is_escalating'] = 1 if recent_amounts == sorted(recent_amounts) else 0
            features['escalation_ratio'] = recent_amounts[-1] / recent_amounts[0] if recent_amounts and recent_amounts[0] > 0 else 1
            
            # Frethatncy change
            if len(customer_history) >= 20:
                first_half_freq = len([t for t in customer_history[:10]])
                second_half_freq = len([t for t in customer_history[-10:]])
                features['freq_change_ratio'] = second_half_freq / first_half_freq if first_half_freq > 0 else 1
            else:
                features['freq_change_ratio'] = 1
            
            # Timand sincand last transaction (horrs)
            if customer_history:
                last_txn = max(customer_history, key=lambda t: t.timestamp)
                features['hours_since_last'] = (now - last_txn.timestamp).total_seconds() / 3600
            else:
                features['hours_since_last'] = 0
        else:
            for key in ['burst_24h', 'is_burst', 'structuring_count', 'is_structuring',
                       'is_escalating', 'escalation_ratio', 'freq_change_ratio', 'hours_since_last']:
                features[key] = 0
        
        # ==================== NETWORK FEATURES (6) ====================
        if network_data:
            features['network_degree'] = network_data.get('degree', 0)
            features['network_betweenness'] = network_data.get('betweenness', 0)
            features['network_pagerank'] = network_data.get('pagerank', 0)
            features['network_clustering'] = network_data.get('clustering', 0)
            features['network_is_hub'] = 1 if features['network_degree'] > 10 else 0
            features['network_community_size'] = network_data.get('community_size', 0)
        else:
            for key in ['network_degree', 'network_betweenness', 'network_pagerank',
                       'network_clustering', 'network_is_hub', 'network_community_size']:
                features[key] = 0
        
        # ==================== RISK AGGREGATION FEATURES (5) ====================
        features['total_risk_flags'] = sum([
            features['is_round_amount'],
            features['is_international'],
            features['any_high_risk'],
            features['is_night'],
            1 if features.get('std_distance', 0) > 3 else 0,
            features.get('is_burst', 0),
            features.get('is_structuring', 0)
        ])
        
        features['amount_risk_score'] = AdvancedFeatureEngineering._calculate_amount_risk(features['amount'])
        features['time_risk_score'] = AdvancedFeatureEngineering._calculate_time_risk(transaction.timestamp)
        features['geo_risk_score'] = (features['origin_high_risk'] + features['destination_high_risk']) / 2
        features['composite_risk'] = (features['amount_risk_score'] + features['time_risk_score'] + features['geo_risk_score']) / 3
        
        return features
    
    @staticmethod
    def _calculate_roundness(amount: float) -> float:
        """Calculates qu√£o 'redondo' √© um valor"""
        if amount % 10000 == 0:
            return 1.0
        elif amount % 5000 == 0:
            return 0.9
        elif amount % 1000 == 0:
            return 0.8
        elif amount % 500 == 0:
            return 0.6
        elif amount % 100 == 0:
            return 0.4
        else:
            return 0.0
    
    @staticmethod
    def _calculate_skewness(data: List[float]) -> float:
        """Calculates skewness (assimetria)"""
        if len(data) < 3:
            return 0
        arr = np.array(data)
        mean = np.mean(arr)
        std = np.std(arr)
        if std == 0:
            return 0
        return np.mean(((arr - mean) / std) ** 3)
    
    @staticmethod
    def _calculate_kurtosis(data: List[float]) -> float:
        """Calculates kurtosis (curtoif)"""
        if len(data) < 4:
            return 0
        arr = np.array(data)
        mean = np.mean(arr)
        std = np.std(arr)
        if std == 0:
            return 0
        return np.mean(((arr - mean) / std) ** 4) - 3
    
    @staticmethod
    def _calculate_amount_risk(amount: float) -> float:
        """Calculates risco baifado no valor"""
        if amount >= 100000:
            return 1.0
        elif amount >= 50000:
            return 0.8
        elif amount >= 10000:
            return 0.5
        else:
            return 0.2
    
    @staticmethod
    def _calculate_time_risk(timestamp: datetime) -> float:
        """Calculates risco baifado no hor√°rio"""
        hour = timestamp.hour
        if hour < 6 or hour > 22:
            return 0.8
        elif hour < 9 or hour > 18:
            return 0.5
        else:
            return 0.2


class MLModelEnsemble:
    """
    Ensemble de m√∫ltiplos modelos ML para detec√ß√£o robusta
    """
    
    def __init__(self):
        logger.info("ü§ñ Initializing ML Model Ensemble...")
        
        # Moofl 1: Isolation Forest (anomaly oftection)
        self.isolation_forest = IsolationForest(
            n_estimators=100,
            contamination=0.05,
            random_state=42
        )
        
        # Moofl 2: Elliptic Envelopand (ortlier oftection)
        self.elliptic_envelope = EllipticEnvelope(
            contamination=0.05,
            random_state=42
        )
        
        # Moofl 3: DBSCAN (ofnsity-baifd clustering)
        self.dbscan = DBSCAN(
            eps=0.5,
            min_samples=5
        )
        
        # Scaler for normalization
        self.scaler = StandardScaler()
        
        # PCA for dimensionality reduction
        self.pca = PCA(n_components=20)
        
        self.is_trained = False
        
        logger.success("‚úÖ ML Model Ensemble initialized")
    
    def train(self, training_data: np.ndarray):
        """Treina os mooflos with data hist√≥ricos"""
        logger.info(f"Training ML models with {len(training_data)} samples...")
        
        # Normalize
        X_scaled = self.scaler.fit_transform(training_data)
        
        # Dimensionality reduction
        X_reduced = self.pca.fit_transform(X_scaled)
        
        # Train moofls
        self.isolation_forest.fit(X_reduced)
        self.elliptic_envelope.fit(X_reduced)
        
        self.is_trained = True
        logger.success("‚úÖ ML models trained successfully")
    
    def predict(self, features: np.ndarray) -> Dict[str, Any]:
        """
        Predi√ß√£o usando ensemble de modelos
        
        Returns:
            Dict com scores de cada modelo e decis√£o final
        """
        if not self.is_trained:
            # Modo Simulated sand n√£o treinado
            return self._simulated_prediction(features)
        
        # Normalizand and reduce
        X_scaled = self.scaler.transform(features.reshape(1, -1))
        X_reduced = self.pca.transform(X_scaled)
        
        # Predictions from each moofl
        results = {}
        
        # Isolation Forest (-1 = anomaly, 1 = normal)
        iso_pred = self.isolation_forest.predict(X_reduced)[0]
        iso_score = self.isolation_forest.score_samples(X_reduced)[0]
        results['isolation_forest'] = {
            'is_anomaly': iso_pred == -1,
            'score': float(iso_score)
        }
        
        # Elliptic Envelope
        ell_pred = self.elliptic_envelope.predict(X_reduced)[0]
        results['elliptic_envelope'] = {
            'is_anomaly': ell_pred == -1,
            'score': 0.8 if ell_pred == -1 else 0.2
        }
        
        # Ensinbland ofcision (voting)
        anomaly_votes = sum([
            results['isolation_forest']['is_anomaly'],
            results['elliptic_envelope']['is_anomaly']
        ])
        
        results['ensemble'] = {
            'is_anomaly': anomaly_votes >= 1,  # Pelo menos 1 modelo detectou
            'confidence': anomaly_votes / 2,
            'anomaly_score': (abs(results['isolation_forest']['score']) + results['elliptic_envelope']['score']) / 2
        }
        
        return results
    
    def _simulated_prediction(self, features: np.ndarray) -> Dict[str, Any]:
        """Predi√ß√£o simulada quando mooflos n√£o est√£o treinados"""
        # Usa heur√≠sticas nas features mais importantes
        feature_dict = {f'f{i}': v for i, v in enumerate(features)}
        
        # Simular oftec√ß√£o baifada in algumas features chave
        risk_indicators = 0
        
        if features[0] > 50000:  # amount alto
            risk_indicators += 1
        if features[3] > 3:  # z-score alto
            risk_indicators += 1
        if features[5] > 0.7:  # roundness alto
            risk_indicators += 1
        
        anomaly_score = min(risk_indicators / 3, 0.9)
        is_anomaly = anomaly_score > 0.5
        
        return {
            'isolation_forest': {'is_anomaly': is_anomaly, 'score': anomaly_score},
            'elliptic_envelope': {'is_anomaly': is_anomaly, 'score': anomaly_score},
            'ensemble': {
                'is_anomaly': is_anomaly,
                'confidence': 0.7,
                'anomaly_score': anomaly_score
            }
        }


class AdvancedMLAgent(BaseAgent):
    """
    Agente ML avan√ßado com ensemble de modelos e 80+ features
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(
            agent_id="advanced_ml_agent",
            agent_type="ml_ensemble",
            config=config
        )
        
        self.feature_engineer = AdvancedFeatureEngineering()
        self.ml_ensemble = MLModelEnsemble()
        
        # Threshold
        self.anomaly_threshold = config.get("anomaly_threshold", 0.7) if config else 0.7
        
        logger.success("‚úÖ Advanced ML Agent initialized with 80+ features")
    
    async def analyze(self, transaction: Transaction, context: Optional[Dict[str, Any]] = None) -> AgentResult:
        """An√°lisand usando ML ensinbland with 80+ features"""
        start_time = time.time()
        
        # Extract features
        customer_history = context.get("customer_history", []) if context else []
        network_data = context.get("network_data", None) if context else None
        
        features_dict = self.feature_engineer.extract_all_features(
            transaction,
            customer_history,
            network_data
        )
        
        # Convert to array
        feature_names = sorted(features_dict.keys())
        features_array = np.array([features_dict[k] for k in feature_names])
        
        # ML prediction
        ml_results = self.ml_ensemble.predict(features_array)
        
        # Parsand results
        is_anomaly = ml_results['ensemble']['is_anomaly']
        anomaly_score = ml_results['ensemble']['anomaly_score']
        confidence = ml_results['ensemble']['confidence']
        
        findings = []
        patterns_detected = []
        
        if is_anomaly:
            findings.append(f"ML Ensemble detected anomaly (score: {anomaly_score:.3f})")
            patterns_detected.append("ml_anomaly")
            
            # Iofntify key contributing features
            top_features = AdvancedMLAgent._identify_top_risk_features(features_dict)
            for feat_name, feat_value in top_features[:5]:
                findings.append(f"High risk feature: {feat_name} = {feat_value:.2f}")
        
        # Additional pattern oftection
        if features_dict.get('is_structuring', 0) > 0:
            patterns_detected.append('structuring')
            findings.append("Structuring pattern detected (multiple txns near threshold)")
        
        if features_dict.get('is_burst', 0) > 0:
            patterns_detected.append('burst')
            findings.append(f"Burst activity: {features_dict.get('burst_24h', 0)} txns in 24h")
        
        if features_dict.get('std_distance', 0) > 3:
            patterns_detected.append('statistical_outlier')
            findings.append(f"Statistical outlier: {features_dict['std_distance']:.1f} std deviations")
        
        execution_time = time.time() - start_time
        
        # Cornt features uifd
        num_features = len(features_dict)
        
        return AgentResult(
            agent_name=self.agent_id,
            agent_type=self.agent_type,
            execution_time=execution_time,
            suspicious=is_anomaly,
            confidence=confidence,
            risk_score=min(anomaly_score, 1.0),
            findings=findings,
            patterns_detected=patterns_detected,
            explanation=f"ML Ensemble analysis with {num_features} features. Anomaly score: {anomaly_score:.3f}",
            evidence={
                "ml_results": ml_results,
                "top_risk_features": dict(AdvancedMLAgent._identify_top_risk_features(features_dict)[:10]),
                "feature_count": num_features
            },
            recommended_action="escalate" if is_anomaly and anomaly_score > 0.8 else "review",
            alert_should_be_created=is_anomaly and anomaly_score > self.anomaly_threshold
        )
    
    @staticmethod
    def _identify_top_risk_features(features: Dict[str, float]) -> List[Tuple[str, float]]:
        """Iofntifies as features with maior valor of risco"""
        risk_features = [
            (name, value) for name, value in features.items()
            if any(keyword in name for keyword in ['risk', 'high', 'suspicious', 'anomaly', 'zscore', 'distance'])
            or value > 0.7
        ]
        return sorted(risk_features, key=lambda x: abs(x[1]), reverse=True)

